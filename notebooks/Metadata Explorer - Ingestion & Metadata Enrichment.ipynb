{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<style>\n",
        "body {\n",
        "    font-family: 'Arial', sans-serif;  /* Change to your desired font */\n",
        "}\n",
        "</style>\n",
        "\n",
        "\n",
        "```\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# Author: pcorreia@google.com\n",
        "```"
      ],
      "metadata": {
        "id": "hpyN8_QD5cIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Enrichment with Gemini ✨\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "clc15c3rxBm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n"
      ],
      "metadata": {
        "id": "xmS3VhBW8ub4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Purpose of this notebook**\n",
        "\n",
        "The purpose of this notebook is to showcase how Gemini works with multi-modal inputs to generate insights and support customers that have either audio, image, or video assets. More info on Gemini's multi-modality [here](https://cloud.google.com/use-cases/multimodal-ai?hl=en).\n",
        "\n",
        "Specifically the scenario we are showcasing in this notebook is the creation of a pipeline that given a set of assets in a Google Cloud Storage bucket is able to create metadata that is accurate and informative about these assets.\n",
        "\n",
        "**Generic Prompts**\n",
        "\n",
        "The prompts are generic and have not been tailored to a specific content type. With further prompt engineering you'd expect to have more detailed metadata generate. For example: if most of your content is sports related, doing prompt that captures specific moments of that sport (red cards, penalties, etc) you'll have richer metadata.\n"
      ],
      "metadata": {
        "id": "JfWKCeBX57X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start"
      ],
      "metadata": {
        "id": "uH327alV8xIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Requirements**\n",
        "\n",
        "Make sure you have the following resources in your GCP environment:\n",
        "\n",
        "\n",
        "*   Google cloud project with the APIs enabled;\n",
        "*   Two Google cloud storage buckets, one for the input files, another to store the resulting assets (such as transcriptions, clips of the original videos and thumbnails)\n",
        "*   Firestore enabled and with a collection created.\n",
        "*   And your media files place in the input bucket.\n",
        "\n",
        "Once you have this place, you'll be able to run the notebook.\n",
        "\n",
        "\n",
        "**Ingestion Pipeline**\n",
        "\n",
        "\n",
        "These are the relevant steps that the notebook will take you on:\n",
        "\n",
        "1.   Load The items from the input bucket;\n",
        "2.   Run the prompts for the audio files;\n",
        "3.   Run the prompts for the image metadata;\n",
        "4.   Run the pipeline for videos (more detail on that section)."
      ],
      "metadata": {
        "id": "bWWYHey78lcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "At the end of this pipeline you now have your GCS output bucket should have the following structure:\n",
        "\n",
        "```bash\n",
        "├── file A\n",
        "│   ├── splits\n",
        "│   │   ├── video split / key moment\n",
        "│   │   ├── wav - audio file for the split\n",
        "│   │   ├── json object with the transcription\n",
        "├── file B\n",
        "│   ├── splits\n",
        "│   │   ├── video split / key moment\n",
        "│   │   ├── wav - audio file for the split\n",
        "│   │   ├── json object with the transcription\n",
        "```\n",
        "The schema for the metadata for each of the asset types is shown in those sections.\n",
        "\n"
      ],
      "metadata": {
        "id": "cpV4vSRootGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up 🛠"
      ],
      "metadata": {
        "id": "hMOTqxS6T9Yi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7Ln01eNJJmN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform google-cloud-speech firebase-admin librosa tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qGmdWacJLnU"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ5O1-5gJP6O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "cJmhHE69UEJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkC7zUXmJUN5"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"editorial-solaris\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL = \"gemini-1.5-flash-001\" # @param {type:\"string\"}\n",
        "INPUT_BUCKET=\"editorial-solaris-media-input\" # @param {type:\"string\"}\n",
        "OUTPUT_BUCKET=\"gs://editorial-solaris-media-output\" # @param {type:\"string\"}\n",
        "#db collection for firestore\n",
        "DB_COLLECTION=\"metadata\"   # @param {type:\"string\"}\n",
        "IMAGE_COLLECTION=\"images\" # @param {type:\"string\"}\n",
        "AUDIO_COLLECTION=\"audio\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "Le6zvMW6Q4rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#common import\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from google.cloud import storage\n",
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#speech imports\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud.speech_v2 import SpeechClient\n",
        "from google.cloud.speech_v2.types import cloud_speech\n",
        "from google.api_core.client_options import ClientOptions\n",
        "import librosa\n",
        "\n",
        "#storage\n",
        "import firebase_admin\n",
        "from firebase_admin import firestore\n",
        "\n",
        "#threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n"
      ],
      "metadata": {
        "id": "XIbKDszhQ6I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Functions"
      ],
      "metadata": {
        "id": "wx9UzEKKUF0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIBKRoL-Iby6"
      },
      "outputs": [],
      "source": [
        "def generate(prompt : list, model :str = MODEL) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 1,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "  return responses.text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your GCS object\n",
        "    # source_blob_name = \"storage-object-name\"\n",
        "\n",
        "    # The path to which the file should be downloaded\n",
        "    # destination_file_name = \"local/path/to/file\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Construct a client side representation of a blob.\n",
        "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
        "    # any content from Google Cloud Storage. As we don't need additional data,\n",
        "    # using `Bucket.blob` is preferred here.\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
        "            source_blob_name, bucket_name, destination_file_name\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "tz2EQESJZNwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json_from_gcs(bucket_name, file_name):\n",
        "    \"\"\"Reads a JSON file from Google Cloud Storage into a Python dictionary.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "        file_name: The name of the JSON file within the bucket.\n",
        "\n",
        "    Returns:\n",
        "        A Python dictionary representing the JSON data, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove 'gs://' prefix if present using replace()\n",
        "    bucket_name = bucket_name.replace(\"gs://\", \"\", 1)  # Replace only the first occurrence\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    try:\n",
        "        contents = blob.download_as_string()\n",
        "        data = json.loads(contents)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading JSON from GCS: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "taf94cMWmuE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_bucket_files_pd(bucket_name):\n",
        "    \"\"\"\n",
        "    Lists files in a GCS bucket with properties (name, size, updated time).\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing file properties.\n",
        "    \"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs()\n",
        "\n",
        "    file_data = []\n",
        "    for blob in blobs:\n",
        "        file_data.append({\n",
        "            'file_name': blob.name,\n",
        "            'size': blob.size,\n",
        "              # Last updated timestamp\n",
        "            'type':blob.content_type,\n",
        "            'created': blob.time_created,\n",
        "            'updated': blob.updated,\n",
        "\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(file_data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "mEKHf-tXk4d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_bucket_files(bucket_name):\n",
        "    \"\"\"\n",
        "    Lists files in a GCS bucket with properties (name, size, updated time).\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "\n",
        "    Returns:\n",
        "        list: List of JSON objects containing file properties.\n",
        "    \"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs()\n",
        "\n",
        "    file_data = []\n",
        "    for blob in blobs:\n",
        "        file_data.append({\n",
        "            'file_name': blob.name,\n",
        "            'size': blob.size,\n",
        "            'type': blob.content_type,\n",
        "            'created': blob.time_created,\n",
        "            'updated': blob.updated,\n",
        "        })\n",
        "\n",
        "    return file_data  # Return the list of JSON objects directly"
      ],
      "metadata": {
        "id": "zQJl4PbdL7P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just removes the long running operations from the files\n",
        "def rename_transcription_files(bucket_name):\n",
        "    \"\"\"Renames files in a GCS bucket using a regular expression.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove 'gs://' prefix if present using replace()\n",
        "    bucket_name = bucket_name.replace(\"gs://\", \"\", 1)  # Replace only the first occurrence\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    blobs = bucket.list_blobs()  # Get a list of all blobs (files) in the bucket\n",
        "\n",
        "    for blob in blobs:\n",
        "        # Check if the file name matches the pattern\n",
        "        if re.match(r\".*_transcript_.*\\.json\", blob.name):\n",
        "            new_name = re.sub(r\"_transcript_.+\\.json\", \"_transcript.json\", blob.name)\n",
        "\n",
        "            # Only rename if the new name is different\n",
        "            if new_name != blob.name:\n",
        "                bucket.rename_blob(blob, new_name)\n",
        "                print(f\"Renamed {blob.name} to {new_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NRgQBLgvkytH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Generation 🤖\n",
        "\n"
      ],
      "metadata": {
        "id": "0O5qHzSiUI3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading assets\n",
        "\n",
        "Create a list of all the objects that are in the input bucket."
      ],
      "metadata": {
        "id": "TeosX91JUV5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = list_bucket_files(INPUT_BUCKET)\n",
        "file_list"
      ],
      "metadata": {
        "id": "XlCI_k8-k9tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio 🎧\n",
        "From the list of all input files, filter out the items that have audio."
      ],
      "metadata": {
        "id": "1Ea-I2b2E3SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Field | Description |\n",
        "|---|---|\n",
        "| **Long Summary** | A detailed and comprehensive summary of the content. |\n",
        "| **Short Summary** | A concise and brief overview of the content. |\n",
        "| **Labels** | Keywords or tags associated with the content. |\n",
        "| **Transcript** | The transcript of the audio content. |"
      ],
      "metadata": {
        "id": "g9N8OmK1vrgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out audio\n",
        "audio_list = []\n",
        "for file in file_list:\n",
        "  if 'audio' in file['type']:\n",
        "    audio_list.append(file)\n",
        "audio_list"
      ],
      "metadata": {
        "id": "U-uiazq8Gl-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary metadata 🧙"
      ],
      "metadata": {
        "id": "XGY3jKfElhuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_audio(prompt : list, model :str = MODEL) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 0.5,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "\n",
        "  return responses.text\n",
        "\n"
      ],
      "metadata": {
        "id": "5WLiCfOFljm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a image metadata\n",
        "def generate_metadata_audio(blob_uri: str, mime_type: str, model:str) -> str:\n",
        "  media_asset = Part.from_uri(\n",
        "      mime_type=mime_type,\n",
        "      uri=blob_uri)\n",
        "\n",
        "  prompt = \"\"\"SYSTEM:```You are a skilled podcast expert. You have a deep understanding of media. Your task is to analyze the provided audio and extract key information.```\n",
        "  INSTRUCTION: Please analyze the following video and provide long summary, short summary, subject topics.Please format your response as a JSON object with the given structure. Avoid any additional comment or text.\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"show_name\" : \"the name of the podcast show\"\n",
        "    \"short_summary\": \"[One paragraph summary of the content]\",\n",
        "    \"long_summary\": \"[two-three paragraph summary of the content]\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_audio(prompt=[media_asset, prompt], model = model )\n",
        "\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "9BTFrYQElmfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(audio : json):\n",
        "  \"\"\"Processes a single row from the audio list.\"\"\"\n",
        "\n",
        "  blob_uri = f\"gs://{INPUT_BUCKET}/{audio['file_name']}\"\n",
        "  try:\n",
        "    result = generate_metadata_audio(blob_uri, audio['type'], MODEL)\n",
        "    response_text = re.sub(r\"json|```\", \"\", result)\n",
        "    audio['metadata'] = json.loads(response_text)\n",
        "    return result\n",
        "  except Exception as e:\n",
        "      print(f\"Error processing {image['file_name']}: {e} \" )\n",
        "      traceback.print_exc()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_audio, audio) for audio in audio_list]\n",
        "\n",
        "    # You can remove the tqdm loop if you don't need progress updates\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Audio Summary Metadata Generated')\n"
      ],
      "metadata": {
        "id": "K7XaoFqglz_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the uris in the object's metadata\n",
        "for audio in audio_list:\n",
        "  audio['name'] = audio['file_name'].split('.')[0]\n",
        "  audio['gcs_uri'] = f\"gs://{INPUT_BUCKET}/{audio['file_name']}\"\n",
        "  formatted_text = json.dumps(audio, indent=4, default=str)\n",
        "  print(formatted_text)"
      ],
      "metadata": {
        "id": "S0wiH4BhmjwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images 📸\n",
        "\n"
      ],
      "metadata": {
        "id": "yoOXJbIfFShD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the images we are simly creating some additional metadata to support future usage, like search or content generation. There's only one step: Run one prompt to get description, photo type, location, subject topics and person\n",
        "\n",
        "\n",
        "The resulting meatada has the following format:\n",
        "\n",
        "| Field | Description |\n",
        "|---|---|\n",
        "| **Description** | A textual description of the image content. |\n",
        "| **Photo Shot Type** | The type of shot used (e.g., close-up, landscape, portrait). |\n",
        "| **Location** | The place where the photo was taken. |\n",
        "| **Labels** | Keywords or tags associated with the image. |\n",
        "| **People** | Names or descriptions of people present in the image. |\n",
        "\n"
      ],
      "metadata": {
        "id": "9VEyOYBdvk5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out videos\n",
        "image_list = []\n",
        "for file in file_list:\n",
        "  if 'image' in file['type']:\n",
        "    image_list.append(file)\n",
        "image_list"
      ],
      "metadata": {
        "id": "iE36IGZoFVNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(prompt : list, model :str = MODEL) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 0.5,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "\n",
        "  return responses.text\n",
        "\n"
      ],
      "metadata": {
        "id": "JFevVJTjaTPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a image metadata\n",
        "def generate_metadata_image(blob_uri: str, mime_type: str, model:str) -> str:\n",
        "  media_asset = Part.from_uri(\n",
        "      mime_type=mime_type,\n",
        "      uri=blob_uri)\n",
        "\n",
        "  prompt = \"\"\"SYSTEM:```You are a skilled image analysis expert. You have a deep understanding of media. Your task is to analyze the provided image and extract key information.```\n",
        "  INSTRUCTION: ```Please analyze the following image and provide a description, subject topics, photo type, persons.Please format your response as a JSON object with the given structure. Avoid any additional comment or text.```\n",
        "  OUTPUT:```=\n",
        "    JSON\n",
        "  {\n",
        "    \"description\": \"[A one line description that would support understanding the contents of the image]\",\n",
        "    \"photo_type\": \"[Type of angles used to take the photography]\",\n",
        "    \"location\" : \"A description of the location where the shot is taken, or if it is a known sight, its name\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "    \"persons\" :\n",
        "  [     { \"person\": \"[person1]\"}, { \"person\": \"[person2]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_image(prompt=[media_asset, prompt], model = model )\n",
        "\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "n-Y4bg5FZZYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image : json):\n",
        "  \"\"\"Processes a single row from the image list.\"\"\"\n",
        "\n",
        "  blob_uri = f\"gs://{INPUT_BUCKET}/{image['file_name']}\"\n",
        "  try:\n",
        "    result = generate_metadata_image(blob_uri, image['type'], MODEL)\n",
        "    # print(f\"Processing {image['file_name']} > {result}\")\n",
        "\n",
        "    response_text = re.sub(r\"json|```\", \"\", result)\n",
        "    image['metadata'] = json.loads(response_text)\n",
        "    return result\n",
        "  except Exception as e:\n",
        "      print(f\"Error processing {image['file_name']} {e}\")\n",
        "\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_image, image) for image in image_list]\n",
        "\n",
        "    # You can remove the tqdm loop if you don't need progress updates\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Images Summary Metadata Generated')\n"
      ],
      "metadata": {
        "id": "eyBUUKbpaDgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in image_list:\n",
        "  image['name'] = image['file_name'].split('.')[0]\n",
        "  image['gcs_uri'] = f\"gs://{INPUT_BUCKET}/{image['file_name']}\"\n"
      ],
      "metadata": {
        "id": "jEWoWzUQagji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in image_list:\n",
        "  formatted_text = json.dumps(image, indent=4, default=str)\n",
        "  print(formatted_text)"
      ],
      "metadata": {
        "id": "i4i5kTqoavwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Videos 🎥"
      ],
      "metadata": {
        "id": "wxH__sURUMPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section creates the metadata for the videos. The relevant steps are:\n",
        "\n",
        "1.   Create the summary metadata with Gemini\n",
        "2.   Ask Gemini to pick the timestamps where relevant thumbnails can be done, and create images out of it.\n",
        "3.   Detect the different key moments in the video, create the cuts of that video.\n",
        "4.   For each of the key moments, create a transcript of the audio.\n",
        "\n",
        "The metadata generated will follow this structure:\n",
        "\n",
        "\n",
        "| Field | Description |\n",
        "|---|---|\n",
        "| **Short Summary** | A concise and brief overview of the video content. |\n",
        "| **Long Summary** | A detailed and comprehensive summary of the video content. |\n",
        "| **Labels** | Keywords or tags associated with the video. |\n",
        "| **Thumbnails** | A list of thumbnails for the video. |\n",
        "| **Key Moments: Shot Type** | The type of shot used for the key moment (e.g., close-up, landscape). |\n",
        "| **Key Moments: Order** | The order or sequence of the key moment within the video. |\n",
        "| **Key Moments: Reason** | The reason why this moment is considered key or important. |\n",
        "| **Key Moments: Start Timestamp** | The timestamp in the video where the key moment starts. |\n",
        "| **Key Moments: End Timestamp** | The timestamp in the video where the key moment ends. |\n",
        "| **Key Moments: Transcript** | The speech-to-text transcript. |"
      ],
      "metadata": {
        "id": "2y2HKifmVK0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out videos\n",
        "video_list = []\n",
        "for file in file_list:\n",
        "  if file['type'] == 'video/mp4':\n",
        "    video_list.append(file)\n",
        "video_list"
      ],
      "metadata": {
        "id": "EYQLXyEelSvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary metadata 🧙\n",
        "This is the function that generates metadata for the asset types that are videos. It is currently not specific to the type of content (i,e: sports highlights versus a press conference highlight) but further refinment of the prompt that is different per content type will yield more accurate data\n"
      ],
      "metadata": {
        "id": "LxhAZyZLfoca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a summary of the video\n",
        "def generate_metadata_video_overview(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media. Your task is to analyze the provided video and extract key information.```\n",
        "  INSTRUCTION: ```Please analyze the following video and provide long summary, short summary, subject topics.Please format your response as a JSON object with the given structure. Avoid any additional comment or text.```\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"short_summary\": \"[One paragraph summary of the content]\",\n",
        "    \"long_summary\": \"[two-three paragraph summary of the content]\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = 'gemini-1.5-pro-001' )\n",
        "  # result_text = generate(prompt=[video1, text1], model = MODEL)\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "TuyKQMthftP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row(video):\n",
        "    \"\"\"Processes a single row from the file list.\"\"\"\n",
        "    blob_uri = f\"gs://{INPUT_BUCKET}/{video['file_name']}\"\n",
        "    result = generate_metadata_video_overview(blob_uri=blob_uri)\n",
        "    response_text = re.sub(r\"json|```\", \"\", result)  # Combined regex for efficiency\n",
        "    summary_metadata = json.loads(response_text)\n",
        "    video['summary'] = summary_metadata\n",
        "    return  summary_metadata\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row, video) for video in video_list]\n",
        "\n",
        "    # You can remove the tqdm loop if you don't need progress updates\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Video Summary Metadata Generated')"
      ],
      "metadata": {
        "id": "u28ioZROoW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the results of the top entry"
      ],
      "metadata": {
        "id": "E2Q6DViXrKn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_json = json.dumps(video_list[0], indent=4, default=str)\n",
        "print(formatted_json)\n"
      ],
      "metadata": {
        "id": "9UkB0Jk-qlw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thumbnails ❇"
      ],
      "metadata": {
        "id": "uDoFJco-Ba4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_thumbnails(video_path, timestamps):\n",
        "    \"\"\"\n",
        "    Extracts thumbnails from a video at specified timestamps using moviepy.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The path to the video file.\n",
        "        timestamps (list): A list of timestamps (in seconds) for the snapshots.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of snapshot images (PIL Image objects).\n",
        "    \"\"\"\n",
        "\n",
        "    clip = VideoFileClip(video_path)\n",
        "\n",
        "    snapshots = [clip.get_frame(t) for t in timestamps]\n",
        "\n",
        "    clip.close()\n",
        "\n",
        "    return snapshots\n"
      ],
      "metadata": {
        "id": "R_JYx7jlBhXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_metadata_video_thumbnails(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media and can accurately identify key moments in a video. Your task is to analyze the provided video and extract key thumbnails.```\n",
        "  INSTRUCTION: ```Give me the timestamp for 3 suitable thumbnails for this video that highlight the key moments.\n",
        "  Do not add any additional text.```\n",
        "  OUTPUT:```\n",
        "  JSON\n",
        "    \"thumbnails\": [\n",
        "      {\n",
        "        \"reason\": \"[Why this would be a suitable thumbnail for the video]\",\n",
        "        \"time\": \"[mm:ss]\",\n",
        "\n",
        "      },\n",
        "      {\n",
        "        \"reason\": \"[Why this would be a suitable thumbnail for the video]\",\n",
        "        \"time\": \"[mm:ss]\",\n",
        "      }\n",
        "    ]\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = 'gemini-1.5-pro-001' )\n",
        "\n",
        "  return result_text\n"
      ],
      "metadata": {
        "id": "lQxOTgl2sZzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_thumbnails(video):\n",
        "    \"\"\"Processes a single row from the video array.\"\"\"\n",
        "    blob_uri = f\"gs://{INPUT_BUCKET}/{video['file_name']}\"\n",
        "    result = generate_metadata_video_thumbnails(blob_uri=blob_uri)\n",
        "    response_text = re.sub(r\"json|```\", \"\", result)  # Combined regex for efficiency\n",
        "    thumbnails_metadata = json.loads(response_text)\n",
        "    video['thumbnails'] = thumbnails_metadata['thumbnails']\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row_thumbnails, video) for video in video_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Video Thumbnails Metadata Generated')"
      ],
      "metadata": {
        "id": "cMSDjspdtE9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_json = json.dumps(video_list, indent=4, default=str)\n",
        "print(formatted_json)\n"
      ],
      "metadata": {
        "id": "bisovW5kwtDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section Metadata 🧙"
      ],
      "metadata": {
        "id": "3mPZOOnPfqpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ6sKkIuI53M"
      },
      "outputs": [],
      "source": [
        "def generate_metadata_video_sections(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media and can accurately identify key moments in a video. Your task is to analyze the provided video and extract all the highlight clips. For each clip, you need to classify the type of highlight and provide the precise start and end timestamps.```\n",
        "  INSTRUCTION: ```Please analyze the following video and provide a list of all the highlight clips with their type and timestamps. Also explain the reason why the selection of that particular timestamp has been made. Please format your response as a JSON object with the given structure. Make sure the audio is not truncated while suggesting the clips. Avoid any additional comment or text.```\n",
        "  OUTPUT:```\n",
        "  JSON\n",
        "  {\n",
        "    \"sections\": [\n",
        "      {\n",
        "        \"type\": \"[highlight type]\",\n",
        "        \"start_time\": \"[mm:ss]\",\n",
        "        \"end_time\": \"[mm:ss]\",\n",
        "        \"reason\" : \"\"\n",
        "      },\n",
        "      {\n",
        "        \"type\":\"[highlight type]\",\n",
        "        \"start_time\": \"[mm:ss]\",\n",
        "        \"end_time\": \"[mm:ss]\",\n",
        "        \"reason\" : \"\"\n",
        "      }\n",
        "    ]\n",
        "  }```\n",
        "  Please make sure the timestamps are accurate and reflect the precise start and end of each highlight clip.\"\"\"\n",
        "\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = 'gemini-1.5-pro-001' )\n",
        "\n",
        "  return result_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row(video):\n",
        "    \"\"\"Processes a single row from the video array, with error handling.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{INPUT_BUCKET}/{video['file_name']}\"\n",
        "        result = generate_metadata_video_sections(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        sections_metadata = json.loads(response_text)\n",
        "        video['sections'] = sections_metadata['sections']\n",
        "    except Exception as e:\n",
        "        # Log or print the error for debugging\n",
        "        print(f\"Error processing video sections: {video['file_name']}: {e}\")\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row, video) for video in video_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Video Sections Metadata Generated')"
      ],
      "metadata": {
        "id": "-EK6NrsUrhyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the result of the top video"
      ],
      "metadata": {
        "id": "4y-nFmhuPLh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_json = json.dumps(video_list[0], indent=4, default=str)\n",
        "print(formatted_json)\n"
      ],
      "metadata": {
        "id": "3pWc-4kUsid2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section Clipping ✂"
      ],
      "metadata": {
        "id": "l-6PHwSCAt2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the folders in the notebook"
      ],
      "metadata": {
        "id": "__vp__gZbzKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output folder if it doesn't exist\n",
        "output_folder = f\"content/\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "#create splits folder if it doesn't exist\n",
        "splits_folder = f\"content/splits\"\n",
        "if not os.path.exists(splits_folder):\n",
        "    os.makedirs(splits_folder)\n",
        "\n",
        "\n",
        "for video in video_list:\n",
        "  #copy file to notebook to start splitting it\n",
        "  destination_file_name = f\"content/{video['file_name']}\"\n",
        "  download_blob(INPUT_BUCKET, video['file_name'], destination_file_name)\n",
        "  #create the splits folders\n",
        "  split_folder = f\"content/splits/{video['file_name'].split('.')[0]}\"\n",
        "  if not os.path.exists(split_folder):\n",
        "    os.makedirs(split_folder)\n"
      ],
      "metadata": {
        "id": "HfpRtAgvPoMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create sections of the original video. Each thread should launch a video split, update the json object and output the file to the local notebook\n"
      ],
      "metadata": {
        "id": "2zQ-d9bxP7Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import *\n",
        "\n",
        "\n",
        "def process_segment(video, index):\n",
        "    source_file_name = f\"content/{video['file_name']}\"\n",
        "    clip = VideoFileClip(source_file_name)\n",
        "\n",
        "    video_name = video['file_name'].split('.')[0]\n",
        "    clip_filename = f\"content/splits/{video_name}/{video_name}_split_{index}.mp4\"\n",
        "    audio_filename = f\"content/splits/{video_name}/{video_name}_split_{index}.wav\"\n",
        "\n",
        "    section_info = video['sections'][index]\n",
        "\n",
        "    #handling section for video\n",
        "    clip_subsection = clip.subclip(section_info[\"start_time\"], section_info[\"end_time\"])\n",
        "    clip_subsection.write_videofile(clip_filename, verbose=False, logger=None)\n",
        "\n",
        "    #handling audio split\n",
        "    audio_clip = clip_subsection.audio\n",
        "    audio_clip.write_audiofile(audio_filename, codec = 'pcm_s16le', verbose=False, logger=None)\n",
        "\n",
        "    #update the uri\n",
        "    gcs_video_uri = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}.mp4\"\n",
        "    gcs_audio_uri = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}.wav\"\n",
        "\n",
        "    video['sections'][index]['split_video_uri'] = gcs_video_uri\n",
        "    video['sections'][index]['split_audio_uri'] = gcs_audio_uri\n",
        "\n",
        "\n",
        "\n",
        "def process_video_segments(video):\n",
        "\n",
        "  #launching threads to do segment cut for one video\n",
        "  with ThreadPoolExecutor() as executor:\n",
        "    # futures = [executor.submit(process_segment, video) for video in video['sections']['highlights']]#TODO: unecessary sections in json object\n",
        "    futures = [executor.submit(process_segment, video, index) for index, row in enumerate(video['sections'])]\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures), desc = f\"Processing {video['file_name']}\"):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "\n",
        "for video in video_list:\n",
        "  process_video_segments(video)\n",
        "\n",
        "\n",
        "print('Video Sections Metadata Generated')"
      ],
      "metadata": {
        "id": "yk6MPJCbQ-Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list[:1]:\n",
        "  # print(video['file_name'])\n",
        "  string = json.dumps(video, indent=4, default=str)\n",
        "  print(string)"
      ],
      "metadata": {
        "id": "818d3a3jeoy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the files from the local folder to the GCS Bucket"
      ],
      "metadata": {
        "id": "RzhQ8573BfRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Move all files in content/splits to a gcs bucket using gsutils\n",
        "!gsutil -m cp -r content/splits {OUTPUT_BUCKET}\n"
      ],
      "metadata": {
        "id": "NSCwQCbs3PlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription 📓\n"
      ],
      "metadata": {
        "id": "_4ifUYKnTsmh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLMSUoQDKPOW"
      },
      "source": [
        "First, you need to initiate a Recognizer which uses the Chirp model and transcribe the audio in English.\n",
        "\n",
        "See [the documentation](https://cloud.google.com/python/docs/reference/speech/latest/google.cloud.speech_v2.types.CreateRecognizerRequest) to learn more about how to configure the `CreateRecognizerRequest` request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7E1VlwDKOfp"
      },
      "outputs": [],
      "source": [
        "client = SpeechClient(\n",
        "    client_options=ClientOptions(api_endpoint=f\"{LOCATION}-speech.googleapis.com\")\n",
        ")\n",
        "\n",
        "language_code = \"en-AU\"\n",
        "recognizer_id = f\"chirp-{language_code.lower()}-interview\"\n",
        "\n",
        "recognizer_request = cloud_speech.CreateRecognizerRequest(\n",
        "    parent=f\"projects/{PROJECT_ID}/locations/{LOCATION}\",\n",
        "    recognizer_id=recognizer_id,\n",
        "    recognizer=cloud_speech.Recognizer(\n",
        "        language_codes=[language_code],\n",
        "        model=\"chirp\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHV7GbFMpAP2"
      },
      "source": [
        "Then, you create a Speech-to-Text [Recognizer](https://cloud.google.com/speech-to-text/v2/docs/recognizers) that uses Chirp running a create operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AarUGGQ_MEn7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    recognizer_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/recognizers/{recognizer_id}\"\n",
        "    recognizer = client.get_recognizer(name=recognizer_name)\n",
        "    print(f\"Recognizer '{recognizer_id}' already exists.\")\n",
        "except Exception:\n",
        "    print(f\"Recognizer '{recognizer_id}' does not exist.\")\n",
        "    create_operation = client.create_recognizer(request=recognizer_request)\n",
        "    recognizer = create_operation.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5W-cuFWMkXA"
      },
      "outputs": [],
      "source": [
        "recognizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create the batch long running request and wait for the results"
      ],
      "metadata": {
        "id": "XgKZ3wTQofak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_segment_transcription(element, client, recognizer):\n",
        "    long_audio_uri = element['split_audio_uri']\n",
        "    directory_path = os.path.dirname(long_audio_uri)\n",
        "\n",
        "    long_audio_config = cloud_speech.RecognitionConfig(\n",
        "      features=cloud_speech.RecognitionFeatures(\n",
        "          enable_automatic_punctuation=True, enable_word_time_offsets=True\n",
        "      ),\n",
        "      auto_decoding_config={},\n",
        "    )\n",
        "\n",
        "\n",
        "    long_audio_request = cloud_speech.BatchRecognizeRequest(\n",
        "        recognizer=recognizer.name,\n",
        "        recognition_output_config={\n",
        "            \"gcs_output_config\": {\"uri\": directory_path}\n",
        "        },\n",
        "        files=[{\"config\": long_audio_config, \"uri\": long_audio_uri}],\n",
        "    )\n",
        "\n",
        "    long_audio_operation = client.batch_recognize(request=long_audio_request)\n",
        "    return long_audio_operation\n",
        "\n",
        "futures = []  # Create an empty list to store the results\n",
        "\n",
        "def process_video_segments_transcription(video):\n",
        "  with ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(process_segment_transcription, section, client, recognizer) for index, section in enumerate(video['sections'])]\n",
        "\n",
        "\n",
        "    # # waiting for threads to complete\n",
        "    # for _ in tqdm.tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {video['file_name']}\"):\n",
        "    #     pass  # No need to process individual results here\n",
        "\n",
        "    for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=f\"Transcribing segments {video['file_name']}\"):\n",
        "      operation = future.result()  # Get the Operation object\n",
        "      # Wait for the operation to complete\n",
        "      response = operation.result()\n",
        "\n",
        "for video in video_list:\n",
        "  process_video_segments_transcription(video)\n",
        "\n",
        "print(f\"Transcriptions done.\")"
      ],
      "metadata": {
        "id": "JmCtThzcaUl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove the long running operations ids from the transcriptions"
      ],
      "metadata": {
        "id": "isecFPoiokJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rename_transcription_files(OUTPUT_BUCKET)"
      ],
      "metadata": {
        "id": "oddhgeKCfgCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the transcriptions information to the overall object.\n",
        "\n",
        "\n",
        "\n",
        "1.   Add the transcription_uri\n",
        "2.   put the accepted transcript in the json object\n",
        "\n"
      ],
      "metadata": {
        "id": "_hgFq4gEopss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list:\n",
        "  for index, segment in enumerate(video['sections']):\n",
        "    video_name = video['file_name'].split('.')[0]\n",
        "    segment['split_transcription_uri'] = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}_transcript.json\"\n",
        "    #read transcription into memory\n",
        "    file_path = segment['split_transcription_uri'].replace(f\"{OUTPUT_BUCKET}/\", \"\")\n",
        "\n",
        "    transcription_results = read_json_from_gcs(OUTPUT_BUCKET, file_path)\n",
        "    final_transcription = \"\"\n",
        "\n",
        "    # Extract and concatenate transcripts\n",
        "    transcripts = []\n",
        "    for result in transcription_results['results']:\n",
        "      if 'alternatives' in result:\n",
        "        for alternative in result['alternatives']:\n",
        "            transcripts.append(alternative['transcript'])\n",
        "    if transcripts:\n",
        "      concatenated_transcript = ' '.join(transcripts)\n",
        "      segment['transcription'] = concatenated_transcript"
      ],
      "metadata": {
        "id": "hELl93irjbbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list[:1]:\n",
        "  string = json.dumps(video, indent=4, default=str)\n",
        "  print(string)"
      ],
      "metadata": {
        "id": "rC4eYhTfo2QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing Metadata 💾"
      ],
      "metadata": {
        "id": "iOW2QOpEpRR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting the firebase db\n",
        "# Initialize Firebase Admin SDK\n",
        "firebase_admin.initialize_app()\n"
      ],
      "metadata": {
        "id": "DaTDafZRiwnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a reference to the Firestore database\n",
        "db = firestore.client()"
      ],
      "metadata": {
        "id": "kt4YJMCNuydy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for audio in tqdm.tqdm(audio_list, desc=\"Storing audio metada\"):\n",
        "  doc_ref = db.collection(AUDIO_COLLECTION).document(audio['name'])\n",
        "  doc_ref.set(audio)"
      ],
      "metadata": {
        "id": "7BXgnwoBcS-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in tqdm.tqdm(image_list, desc=\"Storing images metada\"):\n",
        "  doc_ref = db.collection(IMAGE_COLLECTION).document(image['name'])\n",
        "  doc_ref.set(image)"
      ],
      "metadata": {
        "id": "RH82ANgYTQbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the video information\n",
        "for video in tqdm.tqdm(video_list, desc=\"Storing videos metada\"):\n",
        "    video_name = video['file_name'].split('.')[0]\n",
        "    doc_ref = db.collection(DB_COLLECTION).document(video_name)\n",
        "\n",
        "    # Insert the JSON object into the document\n",
        "    doc_ref.set(video)"
      ],
      "metadata": {
        "id": "_-MmhGGBkS51"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Metadata Enrichment v0.3.ipynb",
      "collapsed_sections": [
        "Le6zvMW6Q4rf",
        "TeosX91JUV5u",
        "1Ea-I2b2E3SL",
        "LxhAZyZLfoca",
        "uDoFJco-Ba4p"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}